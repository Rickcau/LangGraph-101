{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "Now, lets build on top of what you have learned so far.  We will build a **chain** that combines 4 concepts.  In LangChain you can leverage LangChain Expression Language, which is a declarative way to chain LangChain components.\n",
    "\n",
    "In LangChain, a **Chain** is a sequence of steps or components that work together to accomplish a task. Here's a layman's explanation:\n",
    "\n",
    "Think of a Chain like a recipe or a workflow. It's a series of instructions or actions that you want the language model to follow.\n",
    "\n",
    "Each step in the Chain is called a \"Runnable\". This could be things like:\n",
    "\n",
    "- Calling a language model to generate some text\n",
    "- Retrieving information from a database or API\n",
    "- Parsing or transforming the output of a previous step\n",
    "\n",
    "![chain](../../images/chain.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messages\n",
    "LLMs use messages, which we covered in [Module 1](../1-module/2-Messages-Streaming.ipynb).  Messages are used to capture different roles within the conversation.\n",
    "\n",
    "LangChain supports various message types, such as **HumanMessage**, **AIMessage**, **SystemMessage** and **ToolMessage**.  These represent a message from the user, from LLM for the model to instruct behavior, and from a tool call.\n",
    "\n",
    "1. We will start by creating a list of messages.\n",
    "\n",
    "Each message can be suppied with a few things:\n",
    "- **content** - content of the message\n",
    "- **name** - optionally, a message author\n",
    "- **response_metadate** - optionally, a dict of metadata (e.g., often populated by the model provider for **AIMessages**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "messages = [AIMessage(content=f\"So you said you were researching prompt engineering?\", name=\"Model\")]\n",
    "messages.append(HumanMessage(content=f\"Yes, that's right.\",name=\"Rick\"))\n",
    "messages.append(AIMessage(content=f\"Great, what would you like to learn about.\", name=\"Model\"))\n",
    "messages.append(HumanMessage(content=f\"I want to learn about the Plan and Solve pattern\", name=\"Lance\"))\n",
    "\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Models\n",
    "LLMs can use a sequence of messages as input and message types as previously mentioned.  We will leveage Azure OpenAI as an example.\n",
    "\n",
    "Let's check that your **AZURE_OPENAI_API_KEY** is set and if not you will be asked to enter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: 8PVz****************************Isv1\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get root directory path\n",
    "root_dir = Path().absolute().parent.parent\n",
    "env_path = root_dir / '.env'\n",
    "\n",
    "# Load .env from root\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"AZURE_OPENAI_API_KEY\")\n",
    "# Access variables\n",
    "api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "\n",
    "print(f\"API Key: {  api_key[:4] + '*' * 28 + api_key[-4:] }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a model and invoke it with our list of messages.  We can see that the results is an **AIMessage** with specific **response_metadata**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "llm = AzureChatOpenAI(model_name=\"gpt-4o\")\n",
    "result = llm.invoke(messages)\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='One of the best places to see orcas in the United States is the San Juan Islands in Washington State. The waters around these islands are known for having resident orca pods, particularly during the summer months. The southern resident killer whales, which are an endangered population, are often seen in this area.\\n\\nAnother prime location is the Puget Sound, which also offers opportunities to see these magnificent creatures, especially during their feeding season.\\n\\nFor those on the East Coast, while orcas are less common, there have been occasional sightings off the coast of Maine.\\n\\nWhen planning a trip to see orcas, consider joining a guided whale-watching tour, as these are led by experts who know the best spots and times to see orcas while ensuring minimal disturbance to the animals.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 154, 'prompt_tokens': 67, 'total_tokens': 221, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_04751d0b65', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-05904553-bac2-4d1b-b8e6-d3ef04af37cd-0', usage_metadata={'input_tokens': 67, 'output_tokens': 154, 'total_tokens': 221, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the **response_metadata**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.response_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the response_metadata has all sorts of valuable information (e.g. token usage etc.)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools\n",
    "Tools are useful whenever you want to have a model interact with external systems.  External systems (e.g., APIs) often require a particular input schema or payload, rather than natural language.  We we **bind** an API, for example, as a tool we have provided the model with awareness of the required **input schema**.\n",
    "\n",
    "The model will choose to call a tool base dupon the natural language input from the user, and it will return an output that adheres to the tool's schema.  Most LLM providers support tool calling and the [tool calling interface](https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/) in LangChain is fairly simple.\n",
    "\n",
    "You can pass any Python **function** into **Model.bind_tools(function)**. \n",
    "\n",
    "![tool calling](../../images/tool-calling.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a simple example example **tool calling**.  This example is a little different than the example found in the [Messages-Streaming] example found in **module 1**.  \n",
    "\n",
    "We will create a **multiply** function as our tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = llm.bind_tools([multiply])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass an input e.g., **\"What is 2 multipled by 3\" - we will see a tool called returned.\n",
    "\n",
    "The tool call has specific arguments that match the input schema of our function along with the name of the function to call.\n",
    "\n",
    "   ~~~\n",
    "      {'arguments': '{\"a\":2,\"b\":3}', name': 'multiply'}\n",
    "   ~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ZDXzr9C2Wi7ktwgApaZcOFjr', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 62, 'total_tokens': 79, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_04751d0b65', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run-121a001f-7f59-4b7d-b056-32c0b774763a-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_ZDXzr9C2Wi7ktwgApaZcOFjr', 'type': 'tool_call'}], usage_metadata={'input_tokens': 62, 'output_tokens': 17, 'total_tokens': 79, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "tool_call = llm_with_tools.invoke([HumanMessage(content=f\"What is 2 multiplied by 3\", name=\"Lance\")])\n",
    "tool_call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'call_D5knnjVW6yfq1Y9UxPaTeQEx',\n",
       "  'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'},\n",
       "  'type': 'function'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call.additional_kwargs['tool_calls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Messages as State\n",
    "With these foundations and knowledge in place, we can now use **messages** in our graph state.\n",
    "\n",
    "Let's define our state, **MessagesState**, as a **TypedDict** with a single key: **messages**.\n",
    "\n",
    "**messages** is simply a list of messages, as we demostrated eariler (e.g., HumanMessage, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: list[AnyMessage]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducers\n",
    "We have a minor problem!  As we mentioned eariler, each node will return a new value for our state key **messages**, so this means our new value will **override** the prior **messages** state key!\n",
    "\n",
    "As our graph runs, we want to **append** messages to our **messages** state key.  We will use **reducer functions** to address this.  Please read the **Important Notes about Reducers** section below.\n",
    "\n",
    "Reducers allow us to specify how state updates are performed.  If no reducer functiuon is explicitly specified then it is assumed that all updates to that key **should override it**!\n",
    "\n",
    "But, to append messages, we can use the pre-built **add_messagfes** reducer.  This ensures that any **messages** are appended to the existing list of **messages**.\n",
    "\n",
    "We simply need to annotate **messages** key with the **add_messages** reducer function as metadata. \n",
    "\n",
    "## Important Notes about Reducers\n",
    "\n",
    "We can use [**reducer functions**](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) to address this.  Reducers are key to understanding how updates from the nodes are applied to the **State**.  Each key in the **State** has its own independent reducer function.    There are a few different types of reducers, starting with the default tyhpe of reducer:\n",
    "\n",
    "Please take a look at the [**reducers**](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) documentation for a detailed explaination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so let's create a custom state object with two **keys** one called **some_place_holder** which has no **reducer** specified, as a result this is treated as an update to the state, so it is overridden each time.  \n",
    "\n",
    "But, notice how we use an **Annotated** type to specify a reducer function (**operator.add_messages**) for the **messages** key.  If you simply used the **operator.add** as the reducer for **messages** this would allow you to allow append messages only.  By using the (**operator.add_messages**) this allows us to add new messages as well as update existing messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    some_place_holder: str\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a list of messages in graph state is so common, LangGraph has a pre-built **MessagesState**!\n",
    "\n",
    "**MessagesState** is defined as:\n",
    "- With a pre-built single messages key\n",
    "- This is a list of **AnyMessage** objects\n",
    "- It uses the **add_messages** reducer\n",
    "\n",
    "We will usually use **MessagesState** because it is less verbose than defining a customer **TypedDict**, as shown abnove.\n",
    "\n",
    "Let's take a look at using the pre-built **MessagesState*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class MessagesState(MessagesState):\n",
    "    some_place_holder: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go a bit deeper and see how the **add_messages** reducer wrks in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from pprint import pprint\n",
    "from langgraph.graph.message import add_messages\n",
    "# Initial state\n",
    "initial_messages = [AIMessage(content=\"Hello! How can I assist you?\", name=\"Model\"),\n",
    "                    HumanMessage(content=\"I'm looking for information on mountain biking.\", name=\"Rick\")\n",
    "                   ]\n",
    "\n",
    "# New message to add\n",
    "new_message = AIMessage(content=\"Sure, I can help with that. What specifically are you interested in?\", name=\"Model\")\n",
    "\n",
    "# Test\n",
    "result = add_messages(initial_messages , new_message)\n",
    "for m in result:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the new message is added to the **initial_messages**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Graph\n",
    "Now, let's use the **MessagesState** with a graph.  Notice how we did not sub-class the **MessagesState** pre-built state object.  We simply pass it to the **tool_calling_llm** function.  \n",
    "\n",
    "## What is happening here?\n",
    "1. The MessagesState object is passed to the function and by **default** it has a **Messages** key with default **reducer** that looks like this:\n",
    "\n",
    "   ~~~\n",
    "      messages: Annotated[list[AnyMessage], add_messages]\n",
    "   ~~~\n",
    "\n",
    "Which gives us the default ability to add or update messages in the **MessagesState** object as explained eariler.\n",
    "\n",
    "2. Notice that we are invoking our LLM with **llm_with_tools.invoke(state[\"messages\"])**.  This results in the **messages** being passed to the LLM and the response being adding to the **messages** key as a new **AIMessage**!  \n",
    "\n",
    "Very important to note that the have not invoked the *graph* yet, so the tool has **not** been called yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "    \n",
    "# Node\n",
    "def tool_calling_llm(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_edge(\"tool_calling_llm\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass in a *\"Hello\"* and invoke our graph and we will see the the LLM repsonse without anyt tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi there! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\": HumanMessage(content=\"Hello!\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's in a prompt that will result in the LLM choosing a tool to use when it determines that the input or taks requires the functionality provided by a tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_t6fBklbM6n2cneKu1ucPDOzD)\n",
      " Call ID: call_t6fBklbM6n2cneKu1ucPDOzD\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\": HumanMessage(content=\"Multiply 2 and 3\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangGraph-101-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
